"""
Claimify Extractor è©³ç´°æ¸¬è©¦è…³æœ¬

é€™å€‹è…³æœ¬æœƒé¡¯ç¤ºï¼š
1. åŸå§‹æ–‡ç« 
2. åˆ‡åˆ†å¾Œçš„æ‰€æœ‰å¥å­ï¼ˆå«ç·¨è™Ÿå’Œè©•åˆ†ï¼‰
3. ç¯©é¸çµæœï¼ˆå“ªäº›è¢«é¸ä¸­ã€å“ªäº›è¢«éæ¿¾ï¼‰
4. æ¯å€‹å¥å­ç¶“éä¸‰éšæ®µè™•ç†çš„è©³ç´°éç¨‹
5. æœ€çµ‚æå–çš„ claims
"""

import os
import re
from typing import List, Tuple
from dotenv import load_dotenv

load_dotenv()

# æ¸¬è©¦æ–‡ç« 
TEST_ARTICLE = """
Tesla reported record quarterly revenue of $25.5 billion in Q3 2024, representing a 7% increase from the same period last year. CEO Elon Musk stated during the earnings call that "we expect to deliver 2 million vehicles this year."

The company's stock rose 12% following the announcement, making it the best single-day gain since January 2023. Analysts at Morgan Stanley upgraded their price target from $250 to $310, citing strong demand in China where Tesla sold 150,000 vehicles in September alone.

However, some investors remain skeptical about Tesla's ability to maintain growth amid increasing competition from BYD and other Chinese manufacturers. The electric vehicle market is expected to grow significantly in the coming years.

In summary, Tesla's Q3 2024 results exceeded expectations, but challenges lie ahead.
"""

TEST_QUESTION = "What are the key highlights from Tesla's Q3 2024 earnings?"


def print_separator(title: str, char: str = "=", width: int = 70):
    """å°å‡ºåˆ†éš”ç·š"""
    print(f"\n{char * width}")
    print(f" {title}")
    print(f"{char * width}\n")


def test_sentence_splitting():
    """æ¸¬è©¦ 1: é¡¯ç¤ºå¥å­åˆ‡åˆ†çµæœ"""
    from extractor_claimify import ClaimifyExtractor
    
    print_separator("æ¸¬è©¦ 1: å¥å­åˆ‡åˆ†", "=")
    
    extractor = ClaimifyExtractor()
    sentences = extractor._split_into_sentences(TEST_ARTICLE)
    
    print(f"åŸå§‹æ–‡ç« é•·åº¦: {len(TEST_ARTICLE)} å­—å…ƒ")
    print(f"åˆ‡åˆ†å‡º {len(sentences)} å€‹å¥å­:\n")
    
    for i, sent in enumerate(sentences):
        # é¡¯ç¤ºå¥å­é•·åº¦å’Œè©æ•¸
        word_count = len(sent.split())
        print(f"[{i:2d}] ({word_count:2d} è©) {sent[:80]}{'...' if len(sent) > 80 else ''}")
    
    return sentences


def test_prefilter_scoring(sentences: List[str]):
    """æ¸¬è©¦ 2: é¡¯ç¤ºç¯©é¸è©•åˆ†éç¨‹"""
    from extractor_claimify import ClaimifyExtractor
    
    print_separator("æ¸¬è©¦ 2: æ™ºæ…§ç¯©é¸è©•åˆ†", "=")
    
    extractor = ClaimifyExtractor()
    
    print("è©•åˆ†è¦å‰‡:")
    print("  [è·³é] è½‰æŠ˜/ç¸½çµé–‹é ­ã€ä»‹ç´¹å¥ã€æ¨æ¸¬èªæ°£ã€æ„è¦‹ã€é æ¸¬")
    print("  [+3åˆ†] ç™¾åˆ†æ¯”ã€é‡‘é¡ã€å¤§æ•¸å­— (million/billion)")
    print("  [+2åˆ†] å¹´ä»½ã€äººåã€å¼•è¿°è©ã€è·ä½ã€è®ŠåŒ–æ•¸æ“šã€å­£åº¦")
    print("  [+1åˆ†] æœˆä»½ã€é©ä¸­é•·åº¦(10-40è©)")
    print()
    
    results = []
    
    for idx, sent in enumerate(sentences):
        word_count = len(sent.split())
        score = 0
        reasons = []
        skip_reason = None
        
        # æª¢æŸ¥æ˜¯å¦å¤ªçŸ­
        if word_count < 5:
            skip_reason = "å¤ªçŸ­ (< 5 è©)"
        else:
            # æª¢æŸ¥è·³éæ¨¡å¼
            for pattern in extractor.SKIP_PATTERNS:
                if re.search(pattern, sent, re.IGNORECASE):
                    skip_reason = f"ç¬¦åˆè·³éæ¨¡å¼: {pattern[:30]}..."
                    break
            
            if not skip_reason:
                # è¨ˆç®—åˆ†æ•¸
                for pattern, weight in extractor.PRIORITY_PATTERNS:
                    if re.search(pattern, sent, re.IGNORECASE):
                        score += weight
                        # æ‰¾å‡ºåŒ¹é…çš„å…§å®¹
                        match = re.search(pattern, sent, re.IGNORECASE)
                        if match:
                            reasons.append(f"+{weight}: '{match.group()}'")
                
                # é•·åº¦åŠ åˆ†
                if 10 <= word_count <= 40:
                    score += 1
                    reasons.append(f"+1: é©ä¸­é•·åº¦({word_count}è©)")
        
        results.append({
            "idx": idx,
            "sentence": sent,
            "word_count": word_count,
            "score": score,
            "reasons": reasons,
            "skip_reason": skip_reason
        })
    
    # é¡¯ç¤ºçµæœ
    print("-" * 70)
    for r in results:
        status = "âŒ è·³é" if r["skip_reason"] else f"âœ“ {r['score']:2d}åˆ†"
        print(f"[{r['idx']:2d}] {status}")
        print(f"     {r['sentence'][:60]}...")
        
        if r["skip_reason"]:
            print(f"     åŸå› : {r['skip_reason']}")
        elif r["reasons"]:
            print(f"     åŠ åˆ†: {', '.join(r['reasons'])}")
        print()
    
    # é¡¯ç¤ºæ’åºå¾Œçš„çµæœ
    print_separator("ç¯©é¸çµæœ (æŒ‰åˆ†æ•¸æ’åº)", "-")
    
    valid_results = [r for r in results if not r["skip_reason"]]
    valid_results.sort(key=lambda x: (-x["score"], x["idx"]))
    
    print(f"æœ‰æ•ˆå¥å­: {len(valid_results)} / {len(results)}")
    print(f"å¦‚æœ max_sentences=5ï¼Œæœƒé¸æ“‡ä»¥ä¸‹å¥å­:\n")
    
    for i, r in enumerate(valid_results[:5]):
        print(f"  #{i+1} [åŸidx={r['idx']}] {r['score']}åˆ†: {r['sentence'][:50]}...")
    
    return results


def test_full_extraction():
    """æ¸¬è©¦ 3: å®Œæ•´æå–æµç¨‹ï¼ˆå« LLMï¼‰"""
    from extractor_claimify import ClaimifyExtractor
    
    print_separator("æ¸¬è©¦ 3: å®Œæ•´æå–æµç¨‹ (ä½¿ç”¨ LLM)", "=")
    
    extractor = ClaimifyExtractor()
    
    print(f"å•é¡Œ: {TEST_QUESTION}")
    print(f"è¨­å®š: max_sentences=5, use_prefilter=True, max_workers=1")
    print()
    print("é–‹å§‹è™•ç†...")
    print("-" * 70)
    
    # å…ˆæ‰‹å‹•åŸ·è¡Œç¯©é¸ï¼Œä»¥ä¾¿é¡¯ç¤ºæ›´å¤šç´°ç¯€
    all_sentences = extractor._split_into_sentences(TEST_ARTICLE)
    selected = extractor._prefilter_sentences(all_sentences, max_sentences=5)
    
    print(f"\nç¯©é¸çµæœ: å¾ {len(all_sentences)} å¥ä¸­é¸å‡º {len(selected)} å¥")
    print("é¸ä¸­çš„å¥å­:")
    for orig_idx, sent in selected:
        print(f"  [idx={orig_idx}] {sent[:60]}...")
    print()
    
    # åŸ·è¡Œå®Œæ•´æå–
    print("=" * 70)
    print(" é–‹å§‹ä¸‰éšæ®µè™•ç†")
    print("=" * 70)
    
    for i, (orig_idx, sentence) in enumerate(selected):
        print(f"\n{'â”€' * 70}")
        print(f"ğŸ“ å¥å­ {i+1}/{len(selected)} (åŸå§‹ idx={orig_idx})")
        print(f"{'â”€' * 70}")
        print(f"åŸæ–‡: {sentence}")
        print()
        
        # å»ºç«‹ excerpt
        excerpt = extractor._create_excerpt(
            all_sentences, orig_idx,
            extractor.max_preceding,
            extractor.max_following
        )
        print(f"ä¸Šä¸‹æ–‡ (excerpt):")
        print(f"  {excerpt[:100]}...")
        print()
        
        # Stage 1: Selection
        print("ğŸ” Stage 1: Selection")
        contains_verifiable, modified = extractor._stage_selection(
            sentence, excerpt, TEST_QUESTION
        )
        
        if not contains_verifiable:
            print("   çµæœ: âŒ ç„¡å¯é©—è­‰å…§å®¹")
            continue
        
        print(f"   çµæœ: âœ“ åŒ…å«å¯é©—è­‰å…§å®¹")
        if modified and modified != sentence:
            print(f"   ä¿®æ”¹å¾Œ: {modified}")
        else:
            print(f"   (æœªä¿®æ”¹)")
        
        working_sentence = modified or sentence
        print()
        
        # Stage 2: Disambiguation
        print("ğŸ”— Stage 2: Disambiguation")
        can_disambiguate, decontextualized = extractor._stage_disambiguation(
            working_sentence, excerpt, TEST_QUESTION
        )
        
        if not can_disambiguate:
            print("   çµæœ: âŒ ç„¡æ³•æ¶ˆæ­§ç¾©")
            continue
        
        print(f"   çµæœ: âœ“ æ¶ˆæ­§ç¾©æˆåŠŸ")
        if decontextualized and decontextualized != working_sentence:
            print(f"   æ¶ˆæ­§ç¾©å¾Œ: {decontextualized}")
        else:
            print(f"   (æœªä¿®æ”¹)")
        
        final_sentence = decontextualized or working_sentence
        print()
        
        # Stage 3: Decomposition
        print("ğŸ“‹ Stage 3: Decomposition")
        claims = extractor._stage_decomposition(
            final_sentence, excerpt, TEST_QUESTION
        )
        
        if claims:
            print(f"   çµæœ: âœ“ æå–å‡º {len(claims)} å€‹ claims")
            for j, claim in enumerate(claims, 1):
                print(f"   [{j}] {claim}")
        else:
            print("   çµæœ: âŒ æœªæå–å‡º claims")
    
    print()
    print_separator("è™•ç†å®Œæˆ", "=")


def test_comparison():
    """æ¸¬è©¦ 4: åŸæ–‡ vs Claims å°ç…§è¡¨"""
    from extractor_claimify import ClaimifyExtractor
    
    print_separator("æ¸¬è©¦ 4: åŸæ–‡ vs Claims å°ç…§è¡¨", "=")
    
    extractor = ClaimifyExtractor()
    
    result = extractor.extract(
        TEST_ARTICLE,
        question=TEST_QUESTION,
        max_sentences=5,
        use_prefilter=True,
        max_workers=1,
        verbose=False  # é—œé–‰å…§å»º verbose
    )
    
    print("çµ±è¨ˆ:")
    print(f"  - æ–‡ç« ç¸½å¥æ•¸: {result.sentences_total}")
    print(f"  - è¢«ç¯©é¸æ‰: {result.sentences_filtered}")
    print(f"  - å¯¦éš›è™•ç†: {result.sentences_processed}")
    print(f"  - æœ‰ claims: {result.sentences_with_claims}")
    print(f"  - ç„¡å¯é©—è­‰: {result.sentences_no_verifiable}")
    print(f"  - ç„¡æ³•æ¶ˆæ­§ç¾©: {result.sentences_ambiguous}")
    print()
    
    print("è©³ç´°å°ç…§:")
    print("-" * 70)
    
    for i, detail in enumerate(result.claim_details, 1):
        print(f"\n[Claim {i}]")
        print(f"  åŸå¥ (idx={detail['sentence_index']}): ")
        print(f"    {detail['source_sentence'][:70]}...")
        
        if detail['modified_sentence']:
            print(f"  Selection å¾Œ: ")
            print(f"    {detail['modified_sentence'][:70]}...")
        
        if detail['decontextualized_sentence']:
            print(f"  Disambiguation å¾Œ: ")
            print(f"    {detail['decontextualized_sentence'][:70]}...")
        
        print(f"  æœ€çµ‚ Claim: ")
        print(f"    â¡ï¸  {detail['claim']}")


def main():
    """ä¸»ç¨‹å¼"""
    print("\n" + "ğŸš€" * 35)
    print(" CLAIMIFY EXTRACTOR è©³ç´°æ¸¬è©¦")
    print("ğŸš€" * 35)
    
    print("\nğŸ“„ æ¸¬è©¦æ–‡ç« :")
    print("-" * 70)
    print(TEST_ARTICLE.strip())
    print("-" * 70)
    
    # æ¸¬è©¦ 1: å¥å­åˆ‡åˆ†
    sentences = test_sentence_splitting()
    
    # æ¸¬è©¦ 2: ç¯©é¸è©•åˆ†ï¼ˆä¸éœ€è¦ APIï¼‰
    test_prefilter_scoring(sentences)
    
    # è©¢å•æ˜¯å¦ç¹¼çºŒï¼ˆéœ€è¦ APIï¼‰
    print("\n" + "âš ï¸" * 35)
    print(" ä»¥ä¸‹æ¸¬è©¦éœ€è¦å‘¼å« Groq API")
    print("âš ï¸" * 35)
    
    proceed = input("\næ˜¯å¦ç¹¼çºŒ? (y/n): ").strip().lower()
    
    if proceed == 'y':
        # æ¸¬è©¦ 3: å®Œæ•´æµç¨‹
        test_full_extraction()
        
        # æ¸¬è©¦ 4: å°ç…§è¡¨
        test_comparison()
    else:
        print("\nå·²è·³é LLM æ¸¬è©¦ã€‚")
    
    print("\nâœ… æ¸¬è©¦å®Œæˆ!")


if __name__ == "__main__":
    main()
